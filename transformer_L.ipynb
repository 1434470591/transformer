{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Implementation of Transformer in Translation from English to Chinese"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.Import some dependences & set some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import copy\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# here, use `pip install nltk` `python -m nltk.downloader punkt` `python -m nltk.downloader popular`\n",
    "from nltk import word_tokenize\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK = 0                  # unknown word id\n",
    "PAD = 1                  # padding word id\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "LAYERS = 6               # number of encoder & decoder layers\n",
    "H_NUM = 8                # number of multihead attention hidden\n",
    "D_MODEL = 256            # dimentions of embbeding\n",
    "D_FF = 1024              # dimentions of first full connection in feed forward\n",
    "DROPOUT = .1             # rate of dropout\n",
    "MAX_LENGTH = 60          # maxium length of a sentence\n",
    "MAX_WORDS = int(5*1e4)   # maxium words of a dictionary\n",
    "MAX2TRAIN = int(1e6)     # maxium scentences to train\n",
    "# DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0, 1, 2'\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "TRAIN_FILE = './translation2019zh/translation2019zh_train.json'\n",
    "DEV_FILE = './translation2019zh/translation2019zh_valid.json'\n",
    "SAVE_FILE = './model_L.pt'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Data Preparation(tokenize, word2id, add padding & mask, batchnize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_padding(X, padding=0):\n",
    "    '''\n",
    "    Fill 0 into the other of the sentence if the length of it is lower than \n",
    "    ML(the max length of scentence in a batch)\n",
    "    '''\n",
    "    ML = max([len(x) for x in X])\n",
    "    return np.array([\n",
    "        np.concatenate([x, [padding] * (ML - len(x))]) if len(x) < ML else x for x in X\n",
    "    ])\n",
    "\n",
    "\n",
    "def subsequent_mask(size):\n",
    "    '''\n",
    "    Mask out subsequent positions.\n",
    "    '''\n",
    "    mask_shape = (1, size, size)\n",
    "    # generate a matrix with all 1s in the upper-right corner (excluding the main diagonal),\n",
    "    # and 0s in the lower-left corner (including the main diagonal)\n",
    "    # [[0 1 1]\n",
    "    #  [0 0 1]\n",
    "    #  [0 0 0]]\n",
    "    sub_mask = np.triu(np.ones(mask_shape), k=1).astype('uint8')\n",
    "    # return a matrix with all False in the upper-right corner (excluding the main diagonal),\n",
    "    # and True in the lower-left corner (including the main diagonal)\n",
    "    # [[True False False]\n",
    "    #  [True True  False]\n",
    "    #  [True True  True]]\n",
    "    return torch.from_numpy(sub_mask) == 0\n",
    "\n",
    "\n",
    "class Batch:\n",
    "    '''\n",
    "    Object for holding a batch of data with mask during training.\n",
    "    '''\n",
    "    def __init__(self, src, tgt=None, pad=0):\n",
    "        src = torch.from_numpy(src).cuda().long() # [bs, sen_length]\n",
    "        tgt = torch.from_numpy(tgt).cuda().long() # [bs, sen_length]\n",
    "        self.src = src\n",
    "\n",
    "        self.src_mask = (src != pad).unsqueeze(-2) # seq_length -> [1, seq_length]\n",
    "\n",
    "        if tgt is not None:\n",
    "            self.tgt = tgt[:, :-1] # input of decoder excluding <EOS>\n",
    "            self.tgt_y = tgt[:, 1:] # output of decoder excluding <BOS>\n",
    "            self.tgt_mask = self.make_std_mask(self.tgt, pad) # mask some place of target input\n",
    "            self.ntokens = (self.tgt_y != pad).data.sum()\n",
    "\n",
    "    @staticmethod\n",
    "    def make_std_mask(tgt, pad):\n",
    "        '''\n",
    "        Create a mask to hide padding and future words.\n",
    "        '''\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "        tgt_mask = tgt_mask & Variable(subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
    "        return tgt_mask\n",
    "\n",
    "\n",
    "class PrepareData:\n",
    "    '''\n",
    "    txt -> [batch_size, sequence_length]\n",
    "    '''\n",
    "\n",
    "    def __init__(self, train_file, dev_file):\n",
    "        # load data, divide them into source and target, and tokenized them\n",
    "        print('load_data...')\n",
    "        self.train_en, self.train_cn = self.load_data(train_file)\n",
    "        self.dev_en, self.dev_cn = self.load_data(dev_file)\n",
    "\n",
    "        # build word dictionary using train data\n",
    "        print('build_dict...')\n",
    "        self.en_word_dict, self.en_total_words, self.en_index_dict = self.build_dict(self.train_en)\n",
    "        self.cn_word_dict, self.cn_total_words, self.cn_index_dict = self.build_dict(self.train_cn)\n",
    "\n",
    "        # word2id\n",
    "        print('word2id...')\n",
    "        self.train_en_id, self.train_cn_id = self.word2id(self.train_en, self.train_cn, self.en_word_dict, self.cn_word_dict)\n",
    "        self.dev_en_id, self.dev_cn_id = self.word2id(self.dev_en, self.dev_cn, self.en_word_dict, self.cn_word_dict)\n",
    "\n",
    "        # split batch + padding + mask\n",
    "        print('splitBatch...')\n",
    "        self.train_data = self.splitBatch(self.train_en_id, self.train_cn_id, BATCH_SIZE)\n",
    "        self.dev_data = self.splitBatch(self.dev_en_id, self.dev_cn_id, BATCH_SIZE)\n",
    "\n",
    "\n",
    "    def load_data(self, path):\n",
    "        '''\n",
    "        en = [['<BOS>', 'i', 'love', 'you', '<EOS>'], ['<BOS>', 'me', 'too', '<EOS>'], ...]\n",
    "        cn = [['<BOS>', '我', '爱', '你', '<EOS>'], ['<BOS>', '我', '也', '是', '<EOS>'], ...]\n",
    "        where <BOS> means Begin Of Sentence, and <EOS> means End Of Sentence\n",
    "        '''\n",
    "        en, cn = [], []\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            cnt = 0\n",
    "            for line in f:\n",
    "                cnt += 1\n",
    "                line_content = line.replace('\\\\\"', '\\'').split('\"')\n",
    "                # print(line_content) # ['Anyone can do that.', '任何人都可以做到。\\n']\n",
    "                try:\n",
    "                    en.append(['<BOS>'] + word_tokenize(line_content[3]) + ['<EOS>'])\n",
    "                except:\n",
    "                    continue\n",
    "                try:\n",
    "                    cn.append(['<BOS>'] + word_tokenize(' '.join(line_content[7])) + ['<EOS>']) # !!! attention to the space between ' '\n",
    "                except:\n",
    "                    en.pop()\n",
    "                    continue\n",
    "                if (cnt) % MAX2TRAIN == 0:\n",
    "                    print(f'len(en): {len(en)}\\tlen(cn): {len(cn)}')\n",
    "                    break\n",
    "            # print(en)\n",
    "            # print(cn)\n",
    "            print(f'Number of paired data to train/valid is {cnt}')\n",
    "        return en, cn\n",
    "\n",
    "\n",
    "    def build_dict(self, sentences, max_words=MAX_WORDS):\n",
    "        '''\n",
    "        word_dict = {key(word): value(id)}\n",
    "        total_words = len(most_common_words) + 2\n",
    "        index_dict = {key(id): value(word)}\n",
    "        '''\n",
    "        word_count = Counter()\n",
    "        for sentence in sentences:\n",
    "            for w in sentence:\n",
    "                word_count[w] += 1\n",
    "        # just keep most common max_words number words\n",
    "        most_common_words = word_count.most_common(max_words)\n",
    "        total_words = len(most_common_words) + 2 # including <UNK> & <PAD>\n",
    "        print(f'total_words {total_words}')\n",
    "        word_dict = {w[0]: idx + 2 for idx, w in enumerate(most_common_words)}\n",
    "        word_dict['<UNK>'] = UNK\n",
    "        word_dict['<PAD>'] = PAD\n",
    "        index_dict = {v: k for k, v in word_dict.items()}\n",
    "\n",
    "        return word_dict, total_words, index_dict\n",
    "\n",
    "\n",
    "    def word2id(self, en, cn, en_dict, cn_dict, sort=True):\n",
    "        '''\n",
    "        out_en_ids = [[en_dict.get(w, 0) for w in sent] for sent in en]\n",
    "        out_cn_ids = [[cn_dict.get(w, 0) for w in sent] for sent in cn]\n",
    "        and then sorted according the length of sentence in English sentences sequence\n",
    "        '''\n",
    "        out_en_ids = [[en_dict.get(w, 0) for w in sent] for sent in en]\n",
    "        out_cn_ids = [[cn_dict.get(w, 0) for w in sent] for sent in cn]\n",
    "\n",
    "        def len_argsort(seq):\n",
    "            '''\n",
    "            return idx of the sentence in the before sentences sequence sorting from short to long\n",
    "            '''\n",
    "            return sorted(range(len(seq)), key=lambda x: len(seq[x]))\n",
    "\n",
    "        if sort:\n",
    "            sorted_idx = len_argsort(out_en_ids)\n",
    "            out_en_ids = [out_en_ids[i] for i in sorted_idx]\n",
    "            out_cn_ids = [out_cn_ids[i] for i in sorted_idx]\n",
    "\n",
    "        return out_en_ids, out_cn_ids\n",
    "\n",
    "        \n",
    "    def splitBatch(self, en_id, cn_id, batch_size, shuffle=True):\n",
    "        '''\n",
    "        split pairs of en_id/cn_id by specified batch_size\n",
    "        '''\n",
    "        idx_list = np.arange(0, len(en_id), batch_size)\n",
    "        if shuffle: np.random.shuffle(idx_list)\n",
    "\n",
    "        batch_idxs = []\n",
    "        for idx in idx_list:\n",
    "            # in case idx + bs > len(en_id) in the last batch\n",
    "            batch_idxs.append(np.arange(idx, min(idx + batch_size, len(en_id))))\n",
    "\n",
    "        batches = []\n",
    "        for batch_idx in batch_idxs:\n",
    "            batch_en = [en_id[idx] for idx in batch_idx]\n",
    "            batch_cn = [cn_id[idx] for idx in batch_idx]\n",
    "            batch_en = seq_padding(batch_en) # nbatch * batchsize * seqlen\n",
    "            batch_cn = seq_padding(batch_cn)\n",
    "            batches.append(Batch(batch_en, batch_cn))\n",
    "\n",
    "        return batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x11e4cb33cd0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# have a look the matrix of mask\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(subsequent_mask(20)[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Input Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$InputEmbedding(x) = Embedding(x) * \\sqrt {d_{model}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, vocablen):\n",
    "        super(InputEmbedding, self).__init__()\n",
    "        self.emb = nn.Embedding(vocablen, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.emb(x) * math.sqrt(self.d_model)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$PE_{(pos, 2i)} = \\sin (\\frac{pos}{1000^{\\frac{2i}{d_{model}}}})$$\n",
    "$$PE_{(pos, 2i+1)} = \\cos (\\frac{pos}{1000^{\\frac{2i}{d_{model}}}})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    '''\n",
    "    '''\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dp = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model).cuda() # [max_len, d_model]\n",
    "        position = torch.arange(0, max_len).cuda().unsqueeze(1) # [max_len, 1]\n",
    "        div_term = torch.exp(torch.arange(0., d_model, 2).cuda() * -(math.log(10000.0) / d_model))\n",
    "        # pe = torch.zeros(max_len, d_model, device=DEVICE) # [max_len, d_model]\n",
    "        # position = torch.arange(0, max_len, device=DEVICE).unsqueeze(1) # [max_len, 1]\n",
    "        # div_term = torch.exp(torch.arange(0., d_model, 2, device=DEVICE) * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) # PE_(pos, 2i) = sin(pos/(10000^(2i/d_model)))\n",
    "        pe[:, 1::2] = torch.cos(position * div_term) # PE_(pos, 2i+1) = cos(pos/(10000^(2i/d_model)))\n",
    "        pe = pe.unsqueeze(0) # [1, max_len, d_model]\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dp(x + Variable(self.pe[:, :x.size(1)], requires_grad=False))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\tQ &= Linear(X) = X * W_Q\\\\\n",
    "\tK &= Linear(X) = X * W_K\\\\\n",
    "\tV &= Linear(X) = X * W_V\\\\\n",
    "\tAttention(X) &= SelfAttention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is not None: scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = F.softmax(scores, dim=-1)\n",
    "    if dropout is not None: p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h # No. of heads\n",
    "        # 4 full connect function as WQ, WK, WV, multihead concat matrix\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dp = nn.Dropout(p=dropout)\n",
    "\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        if mask is not None: mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "        # [bs, h, len, dim/h]\n",
    "        q, k, v = [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "                    for l, x in zip(self.linears, (query, key, value))]\n",
    "        x, self.attn = attention(q, k, v, mask=mask, dropout=self.dp)\n",
    "        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.LayerNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$LayerNorm(x) = \\alpha * \\frac{x_{ij} - \\mu_{i}}{\\sqrt{\\sigma _i^2 + \\epsilon}} + \\beta$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        # init all alpha = 1, all beta = 0\n",
    "        self.alpha = nn.Parameter(torch.ones(features))\n",
    "        self.beta = nn.Parameter(torch.zeros(features))\n",
    "        # smooth\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.alpha * (x - mean) / torch.sqrt(std ** 2 + self.eps) + self.beta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.Positionwise FeedForward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$PositionwiseFeedForward(X_{attn}) = Linear(Activate(Linear(X_{attn})))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(self.act(self.w_1(x))))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.Utilities class: SublayerConnection & clones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$SublayerConnection(X) = X + SubLayer(X)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    '''\n",
    "    First, connect x and norm\n",
    "    Finally, connect MultiHeadAttention and PositionwiseFeedForward\n",
    "    '''\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "        # return x + self.dropout(self.norm(sublayer(x))) # ?\n",
    "\n",
    "\n",
    "def clones(module, N):\n",
    "    '''\n",
    "    clone the module not sharing the parameters\n",
    "    '''\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.EncoderLayer & Encoder(N_head EncoderLayers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        # embedding -> Multi head Attention\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        # attn -> feedforward\n",
    "        return self.sublayer[1](x, self.feed_forward)\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    # layer = EncoderLayer\n",
    "    # N = 6\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        # clone N encoder_layers\n",
    "        self.layers = clones(layer, N)\n",
    "        # LayerNorm for encoder_layer\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # Attention: mask for x\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.DecoderLayer & Decoder(N_head DecoderLayers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.size = size\n",
    "        # Self-Attention\n",
    "        self.self_attn = self_attn\n",
    "        # Context-Attention with context from encoder\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        # 1.sef-attention, 2.context-attention, 3.feedforward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        # m save the result of hidden of encoder\n",
    "        m = memory        \n",
    "        # Self-Attention：q, k, v from decoder hidden\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        # Context-Attention：q from decoder hidden，k, v from encoder hidden\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
    "        return self.sublayer[2](x, self.feed_forward)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    # layer = DecoderLayer\n",
    "    # N = 6\n",
    "    def __init__(self, layer, N):\n",
    "        super(Decoder, self).__init__()\n",
    "        # clone N decoder_layer\n",
    "        self.layers = clones(layer, N)\n",
    "        # LayerNorm for decoder_layer\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        # Attention: mask for src & tgt\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://pic1.zhimg.com/80/v2-4b53b731a961ee467928619d14a5fd44_720w.jpg' text-align='center'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.1.Encoder\n",
    "\n",
    "- 1).InputEmbedding + PositionalEncoding\n",
    "\n",
    "$$X_{emb} = InputEmbedding(X) + PositionalEncoding(pos)$$\n",
    "\n",
    "- 2).MultiHeadSelfAttention\n",
    "\n",
    "$$X_{attn} = MHA(X_{emb}) = Concatenate(Attention_i(X_{emb})) * W_C$$\n",
    "$$i = [1, numheads]$$\n",
    "$$\n",
    "\\begin{align*}\n",
    "\tQ &= Linear(X) = X * W_Q\\\\\n",
    "\tK &= Linear(X) = X * W_K\\\\\n",
    "\tV &= Linear(X) = X * W_V\n",
    "\\end{align*}\n",
    "$$\n",
    "$$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$\n",
    "\n",
    "- 3).SublayerConnection + Norm\n",
    "\n",
    "$$X_{attn} = LayerNorm(X_{attn})$$\n",
    "$$X_{attn} = X + X_{attn}$$\n",
    "\n",
    "- 4).PositionwiseFeedForward\n",
    "\n",
    "$$X_{hidden} = Linear(Activate(Linear(X_{attn})))$$\n",
    "\n",
    "- 5).Repeat 3)\n",
    "\n",
    "$$X_{hidden} = LayerNorm(X_{hidden})$$\n",
    "$$X_{hidden} = X_{attn} + X_{hidden}$$\n",
    "\n",
    "- 6).Repeat 2) ~ 5) * N \n",
    "\n",
    "\tLet the output of previous 5) be the input of next 2), repeating N times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.2.Decoder\n",
    "\n",
    "- 1).InputEmbedding + PositionalEncoding\n",
    "\n",
    "$$Y_{emb} = InputEmbedding(Y) + PositionalEncoding(pos)$$\n",
    "\n",
    "- 2).MultiHeadSelfAttention\n",
    "\n",
    "$$Y_{attn1} = MaskedMHA(Y_{emb}) = Concatenate(Attention_i(Y_{emb})) * W_C$$\n",
    "$$i = [1, numheads]$$\n",
    "$$\n",
    "\\begin{align*}\n",
    "\tQ &= Linear(Y) = Y * W_Q\\\\\n",
    "\tK &= Linear(Y) = Y * W_K\\\\\n",
    "\tV &= Linear(Y) = Y * W_V\n",
    "\\end{align*}\n",
    "$$\n",
    "$$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$\n",
    "\n",
    "- 3).SublayerConnection + Norm\n",
    "\n",
    "$$Y_{attn1} = LayerNorm(Y_{attn1})$$\n",
    "$$Y_{attn1} = Y + Y_{attn1}$$\n",
    "\n",
    "- 4).MultiHeadContextAttention\n",
    "\n",
    "$$Y_{attn2} = MHA(Y_{attn1}, M) = Concatenate(Attention_i(Y_{attn1}, M)) * W_C$$\n",
    "$$i = [1, numheads], M = X_{hidden}$$\n",
    "$$\n",
    "\\begin{align*}\n",
    "\tQ &= Linear(Y_{attn1}) = Y_{attn1} * W_Q\\\\\n",
    "\tK &= Linear(M) = X_{hidden} * W_K\\\\\n",
    "\tV &= Linear(M) = X_{hidden} * W_V\n",
    "\\end{align*}\n",
    "$$\n",
    "$$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$\n",
    "\n",
    "- 5).Repeat 3)\n",
    "\n",
    "$$Y_{attn2} = LayerNorm(Y_{attn2})$$\n",
    "$$Y_{attn2} = Y + Y_{attn2}$$\n",
    "\n",
    "- 6).PositionwiseFeedForward\n",
    "\n",
    "$$Y_{hidden} = Linear(Activate(Linear(Y_{attn2})))$$\n",
    "\n",
    "- 7).Repeat 3)\n",
    "\n",
    "$$Y_{hidden} = LayerNorm(Y_{hidden})$$\n",
    "$$Y_{hidden} = Y_{attn2} + Y_{hidden}$$\n",
    "\n",
    "- 8).Repeat 2) ~ 7) * N \n",
    "\n",
    "\tLet the output of previous 7) be the input of next 2), repeating N times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.3.linear + log_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    # vocab: tgt_vocab\n",
    "    def __init__(self, d_model, vocab_len):\n",
    "        super(Generator, self).__init__()\n",
    "        # let the result of decoder be the size from d_model to vocab_len\n",
    "        self.proj = nn.Linear(d_model, vocab_len)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # softmax + log\n",
    "        return F.log_softmax(self.proj(x), dim=-1)\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator \n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "\n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        # let the result of encoder as the input parameter(memory) of decoder\n",
    "        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.Make a real Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(src_vocab_len, tgt_vocab_len, N=LAYERS, h=H_NUM, d_model=D_MODEL*2, d_ff=D_FF*2, dropout=DROPOUT):\n",
    "    '''\n",
    "    Instantiation model\n",
    "        param:\n",
    "            src_vocab, tgt_vocab: word_dict = {key(word): value(id)}\n",
    "            src_vocab_len, tgt_vocab_len: len of src_vocab, tgt_vocab\n",
    "            N: # of layers in encoder or decoder\n",
    "            h: # of heads in multi-head attention\n",
    "            d_model: dimentions of embbeding\n",
    "            d_ff: dimentions of first full connection in feed forward\n",
    "            dropout: rate of dropout\n",
    "        return:\n",
    "            model\n",
    "    '''\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadAttention(h, d_model).cuda()\n",
    "    pe = PositionalEncoding(d_model, dropout).cuda()\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout).cuda()\n",
    "\n",
    "    model = nn.DataParallel(Transformer(\n",
    "        encoder=Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout).cuda(), N).cuda(),\n",
    "        decoder=Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout).cuda(), N).cuda(),\n",
    "        src_embed=nn.Sequential(InputEmbedding(d_model, src_vocab_len).cuda(), c(pe)),\n",
    "        tgt_embed=nn.Sequential(InputEmbedding(d_model, tgt_vocab_len).cuda(), c(pe)),\n",
    "        generator=Generator(d_model, tgt_vocab_len)\n",
    "    )).cuda()\n",
    "\n",
    "    # model = Transformer(\n",
    "    #     encoder=Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout).cuda(), N).cuda(),\n",
    "    #     decoder=Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout).cuda(), N).cuda(),\n",
    "    #     src_embed=nn.Sequential(InputEmbedding(d_model, src_vocab_len).cuda(), c(pe)),\n",
    "    #     tgt_embed=nn.Sequential(InputEmbedding(d_model, tgt_vocab_len).cuda(), c(pe)),\n",
    "    #     generator=Generator(d_model, tgt_vocab_len)\n",
    "    # ).cuda()\n",
    "\n",
    "    # This was important from the code. \n",
    "    # Initialize parameters with Glorot / fan_avg.\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            # use nn.init.xavier_uniform\n",
    "            nn.init.xavier_uniform_(p)\n",
    "\n",
    "    return model.cuda()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.Smooth the label(implement by KLdivloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    '''\n",
    "    smooth the label\n",
    "    '''\n",
    "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.size = size\n",
    "        self.smoothing = smoothing\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.padding_idx = padding_idx\n",
    "        self.true_dist = None\n",
    "        self.criterion = nn.KLDivLoss(reduction='sum')\n",
    "        \n",
    "    def forward(self, x, target):\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = x.data.clone()\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        true_dist[:, self.padding_idx] = 0\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        if mask.dim() > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        self.true_dist = true_dist\n",
    "        return self.criterion(x, Variable(true_dist, requires_grad=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.Compute the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossComputing:\n",
    "    '''\n",
    "    compute loss and update the parameters by backpropagation\n",
    "    '''\n",
    "    def __init__(self, generator, criterion, optim=None):\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion\n",
    "        self.optim = optim\n",
    "\n",
    "    def __call__(self, pred, gt, ntokens):\n",
    "        x = self.generator(pred)\n",
    "        y = gt\n",
    "        loss = self.criterion(x.contiguous().view(-1, x.size(-1)),\n",
    "                              y.contiguous().view(-1)) / ntokens\n",
    "        loss.backward()\n",
    "        if self.optim is not None:\n",
    "            self.optim.step()\n",
    "            self.optim.optimizer.zero_grad()\n",
    "        return loss.data.item() * ntokens.float()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.Set optimizer with a warmupdown learning rate\n",
    "\n",
    "$$lr = lr_{base} * [d_{model}^{-.5} * \\min{(step\\_ num^{-.5}, step\\_ num*warmup\\_  steps^{-1.5})}]$$\n",
    "\n",
    "The lr increases linearly with a fixed warmup_steps, decreases proportional to the inverse square root of step_num when it reached warmup_steps(here is 4000).\n",
    "\n",
    "The base optimizer is Adam with beta1=.9, beta2=.98, epsilon=1e-9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoamOpt:\n",
    "    '''\n",
    "    Optim wrapper that implements rate.\n",
    "    '''\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        self.model_size = model_size  # d_model\n",
    "        self.factor = factor          # lr_base\n",
    "        self.warmup = warmup          # warmup_steps\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self._rate = 0\n",
    "\n",
    "    def rate(self, step=None):\n",
    "        'Implement the lr above'\n",
    "        if step is None: step = self._step\n",
    "        return self.factor * (self.model_size ** (-.5) * min(step ** (-.5), step * self.warmup ** (-1.5)))\n",
    "\n",
    "    def step(self):\n",
    "        '''\n",
    "        update parameters and rate\n",
    "        '''\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups: p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "        \n",
    "\n",
    "def get_std_opt(model):\n",
    "\n",
    "    return NoamOpt(\n",
    "        model_size=model.src_embed[0].d_model,\n",
    "        factor=2,\n",
    "        warmup=4000,\n",
    "        optimizer=torch.optim.Adam(model.parameters(), lr=0, betas=(.9, .98), eps=1e-9)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.Train and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(data, model, loss_compute, epoch):\n",
    "    '''\n",
    "    run one epoch of train\n",
    "    '''\n",
    "\n",
    "    start = datetime.now()\n",
    "    total_loss = 0.\n",
    "    tokens = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    datalen = len(data)\n",
    "    roundnum = datalen // (10 ** math.floor(math.log10(datalen))) * (10 ** math.floor(math.log10(datalen)))\n",
    "    steplen = roundnum // 4\n",
    "\n",
    "    for i, batch in enumerate(data):\n",
    "        out = model(batch.src, batch.tgt, batch.src_mask, batch.tgt_mask)\n",
    "        loss = loss_compute(out, batch.tgt_y, batch.ntokens)\n",
    "\n",
    "        total_loss += loss\n",
    "        tokens += batch.ntokens\n",
    "        total_tokens += batch.ntokens\n",
    "\n",
    "        # if (i + 1) % 50 == 0:\n",
    "        if (i + 1) % steplen == 0:\n",
    "            elapsed = datetime.now() - start\n",
    "            print(f'Epoch: {epoch+1} / {EPOCHS} Batch: {i+1} Loss: {loss / batch.ntokens} Tokens per Sec: {tokens.float() / elapsed.total_seconds()}')\n",
    "            start = datetime.now()\n",
    "            tokens = 0\n",
    "    \n",
    "    return total_loss / total_tokens\n",
    "\n",
    "\n",
    "def train(data, model, criterion, optimizer):\n",
    "    '''\n",
    "    run EPOCHS epoch of train\n",
    "    '''\n",
    "\n",
    "    print(\">>>>>>>>>> Start training...\\n\")\n",
    "    train_start = datetime.now()\n",
    "    best_dev_loss = 1e5 # init a higher dev_loss\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "\n",
    "        epoch_start = datetime.now()\n",
    "\n",
    "        model.train()\n",
    "        run_epoch(data.train_data, model, LossComputing(model.module.generator, criterion, optimizer), epoch)\n",
    "        # run_epoch(data.train_data, model, LossComputing(model.generator, criterion, optimizer), epoch)\n",
    "\n",
    "        model.eval()\n",
    "        print('>>>>> Evaluate...')\n",
    "        dev_loss = run_epoch(data.dev_data, model, LossComputing(model.module.generator, criterion, None), epoch)\n",
    "        # dev_loss = run_epoch(data.dev_data, model, LossComputing(model.generator, criterion, None), epoch)\n",
    "        print(f'>>>>> Evaluate loss {dev_loss}')\n",
    "        \n",
    "        if dev_loss < best_dev_loss:\n",
    "            torch.save(model.state_dict(), SAVE_FILE)\n",
    "            best_dev_loss = dev_loss\n",
    "\n",
    "        print(f'One epoch cost {datetime.now()-epoch_start}.\\n')\n",
    "    \n",
    "    print(f'<<<<<<<<<< Finished training, cost {datetime.now()-train_start}.')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_data...\n",
      "len(en): 1000000\tlen(cn): 1000000\n",
      "Number of paired data to train/valid is 1000000\n",
      "Number of paired data to train/valid is 39323\n",
      "build_dict...\n",
      "total_words 50002\n",
      "total_words 8204\n",
      "word2id...\n",
      "splitBatch...\n",
      "src_vocab_len: 50002\n",
      "tgt_vocab_len: 8204\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "data = PrepareData(TRAIN_FILE, DEV_FILE)\n",
    "\n",
    "src_vocab_len = len(data.en_word_dict)\n",
    "tgt_vocab_len = len(data.cn_word_dict)\n",
    "\n",
    "en_word_dict = data.en_word_dict\n",
    "\n",
    "print(f'src_vocab_len: {src_vocab_len}')\n",
    "print(f'tgt_vocab_len: {tgt_vocab_len}')\n",
    "\n",
    "# init model\n",
    "model = make_model(\n",
    "    src_vocab_len=src_vocab_len,\n",
    "    tgt_vocab_len=tgt_vocab_len,\n",
    "    N=LAYERS,\n",
    "    h=H_NUM,\n",
    "    d_model=D_MODEL,\n",
    "    d_ff=D_FF,\n",
    "    dropout=DROPOUT\n",
    ")\n",
    "\n",
    "criterion = LabelSmoothing(tgt_vocab_len, padding_idx=0, smoothing=0.0)\n",
    "optimizer = NoamOpt(\n",
    "    model_size=D_MODEL,\n",
    "    factor=1,\n",
    "    warmup=4000,\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=0, betas=(.9, .98), eps=1e-9)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>>>> Start training...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\siat\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 / 20 Batch: 7500 Loss: 3.91288423538208 Tokens per Sec: 5880.43115234375\n",
      "Epoch: 1 / 20 Batch: 15000 Loss: 3.599436044692993 Tokens per Sec: 5834.32080078125\n",
      "Epoch: 1 / 20 Batch: 22500 Loss: 3.337946891784668 Tokens per Sec: 5809.421875\n",
      "Epoch: 1 / 20 Batch: 30000 Loss: 3.017076253890991 Tokens per Sec: 5627.42919921875\n",
      ">>>>> Evaluate...\n",
      "Epoch: 1 / 20 Batch: 250 Loss: 2.609365224838257 Tokens per Sec: 6593.0498046875\n",
      "Epoch: 1 / 20 Batch: 500 Loss: 3.1945793628692627 Tokens per Sec: 6552.2958984375\n",
      "Epoch: 1 / 20 Batch: 750 Loss: 2.597550630569458 Tokens per Sec: 7050.60986328125\n",
      "Epoch: 1 / 20 Batch: 1000 Loss: 2.902169704437256 Tokens per Sec: 6304.05712890625\n",
      ">>>>> Evaluate loss 2.885564088821411\n",
      "One epoch cost 1:48:46.148399.\n",
      "\n",
      "Epoch: 2 / 20 Batch: 7500 Loss: 2.917682647705078 Tokens per Sec: 5674.03564453125\n",
      "Epoch: 2 / 20 Batch: 15000 Loss: 3.0629422664642334 Tokens per Sec: 5734.935546875\n",
      "Epoch: 2 / 20 Batch: 22500 Loss: 2.952914237976074 Tokens per Sec: 5686.22705078125\n",
      "Epoch: 2 / 20 Batch: 30000 Loss: 2.7176668643951416 Tokens per Sec: 5694.38623046875\n",
      ">>>>> Evaluate...\n",
      "Epoch: 2 / 20 Batch: 250 Loss: 2.398094892501831 Tokens per Sec: 6731.06494140625\n",
      "Epoch: 2 / 20 Batch: 500 Loss: 2.9790852069854736 Tokens per Sec: 6781.33154296875\n",
      "Epoch: 2 / 20 Batch: 750 Loss: 2.32419490814209 Tokens per Sec: 7335.12451171875\n",
      "Epoch: 2 / 20 Batch: 1000 Loss: 2.616023302078247 Tokens per Sec: 6509.54833984375\n",
      ">>>>> Evaluate loss 2.6177401542663574\n",
      "One epoch cost 1:50:08.316464.\n",
      "\n",
      "Epoch: 3 / 20 Batch: 7500 Loss: 2.694042444229126 Tokens per Sec: 5726.6201171875\n",
      "Epoch: 3 / 20 Batch: 15000 Loss: 2.8866100311279297 Tokens per Sec: 5819.60205078125\n",
      "Epoch: 3 / 20 Batch: 22500 Loss: 2.816208600997925 Tokens per Sec: 6065.66650390625\n",
      "Epoch: 3 / 20 Batch: 30000 Loss: 2.5747690200805664 Tokens per Sec: 5967.86181640625\n",
      ">>>>> Evaluate...\n",
      "Epoch: 3 / 20 Batch: 250 Loss: 2.2693819999694824 Tokens per Sec: 6968.330078125\n",
      "Epoch: 3 / 20 Batch: 500 Loss: 2.860192060470581 Tokens per Sec: 7669.99169921875\n",
      "Epoch: 3 / 20 Batch: 750 Loss: 2.249553918838501 Tokens per Sec: 7218.13427734375\n",
      "Epoch: 3 / 20 Batch: 1000 Loss: 2.4895336627960205 Tokens per Sec: 7011.64306640625\n",
      ">>>>> Evaluate loss 2.505232095718384\n",
      "One epoch cost 1:46:21.471929.\n",
      "\n",
      "Epoch: 4 / 20 Batch: 7500 Loss: 2.5824387073516846 Tokens per Sec: 6092.70263671875\n",
      "Epoch: 4 / 20 Batch: 15000 Loss: 2.7922773361206055 Tokens per Sec: 5970.75\n",
      "Epoch: 4 / 20 Batch: 22500 Loss: 2.74790620803833 Tokens per Sec: 5683.4921875\n",
      "Epoch: 4 / 20 Batch: 30000 Loss: 2.4917166233062744 Tokens per Sec: 5673.58740234375\n",
      ">>>>> Evaluate...\n",
      "Epoch: 4 / 20 Batch: 250 Loss: 2.218923330307007 Tokens per Sec: 7521.28955078125\n",
      "Epoch: 4 / 20 Batch: 500 Loss: 2.7796943187713623 Tokens per Sec: 6728.6982421875\n",
      "Epoch: 4 / 20 Batch: 750 Loss: 2.2049522399902344 Tokens per Sec: 6690.123046875\n",
      "Epoch: 4 / 20 Batch: 1000 Loss: 2.4292421340942383 Tokens per Sec: 6537.59619140625\n",
      ">>>>> Evaluate loss 2.439203977584839\n",
      "One epoch cost 1:47:23.862318.\n",
      "\n",
      "Epoch: 5 / 20 Batch: 7500 Loss: 2.523906946182251 Tokens per Sec: 5755.6748046875\n",
      "Epoch: 5 / 20 Batch: 15000 Loss: 2.7210495471954346 Tokens per Sec: 5683.16162109375\n",
      "Epoch: 5 / 20 Batch: 22500 Loss: 2.700852632522583 Tokens per Sec: 5669.4189453125\n",
      "Epoch: 5 / 20 Batch: 30000 Loss: 2.4215712547302246 Tokens per Sec: 5660.59228515625\n",
      ">>>>> Evaluate...\n",
      "Epoch: 5 / 20 Batch: 250 Loss: 2.1668126583099365 Tokens per Sec: 7310.3291015625\n",
      "Epoch: 5 / 20 Batch: 500 Loss: 2.7691729068756104 Tokens per Sec: 6559.41162109375\n",
      "Epoch: 5 / 20 Batch: 750 Loss: 2.133897066116333 Tokens per Sec: 6554.916015625\n",
      "Epoch: 5 / 20 Batch: 1000 Loss: 2.3726961612701416 Tokens per Sec: 7052.3837890625\n",
      ">>>>> Evaluate loss 2.3938424587249756\n",
      "One epoch cost 1:50:17.740915.\n",
      "\n",
      "Epoch: 6 / 20 Batch: 7500 Loss: 2.473029851913452 Tokens per Sec: 5700.7666015625\n",
      "Epoch: 6 / 20 Batch: 15000 Loss: 2.707719564437866 Tokens per Sec: 5713.369140625\n",
      "Epoch: 6 / 20 Batch: 22500 Loss: 2.654679775238037 Tokens per Sec: 5670.84521484375\n",
      "Epoch: 6 / 20 Batch: 30000 Loss: 2.40629506111145 Tokens per Sec: 5943.00146484375\n",
      ">>>>> Evaluate...\n",
      "Epoch: 6 / 20 Batch: 250 Loss: 2.139272928237915 Tokens per Sec: 6932.98095703125\n",
      "Epoch: 6 / 20 Batch: 500 Loss: 2.7256054878234863 Tokens per Sec: 7158.53125\n",
      "Epoch: 6 / 20 Batch: 750 Loss: 2.1205341815948486 Tokens per Sec: 7933.6650390625\n",
      "Epoch: 6 / 20 Batch: 1000 Loss: 2.336354970932007 Tokens per Sec: 6705.54150390625\n",
      ">>>>> Evaluate loss 2.3583714962005615\n",
      "One epoch cost 1:48:50.834656.\n",
      "\n",
      "Epoch: 7 / 20 Batch: 7500 Loss: 2.4730143547058105 Tokens per Sec: 6002.9794921875\n",
      "Epoch: 7 / 20 Batch: 15000 Loss: 2.6197612285614014 Tokens per Sec: 6559.7421875\n",
      "Epoch: 7 / 20 Batch: 22500 Loss: 2.5993149280548096 Tokens per Sec: 6239.70263671875\n",
      "Epoch: 7 / 20 Batch: 30000 Loss: 2.338526725769043 Tokens per Sec: 5708.97705078125\n",
      ">>>>> Evaluate...\n",
      "Epoch: 7 / 20 Batch: 250 Loss: 2.1113829612731934 Tokens per Sec: 7275.03369140625\n",
      "Epoch: 7 / 20 Batch: 500 Loss: 2.7021312713623047 Tokens per Sec: 8060.77783203125\n",
      "Epoch: 7 / 20 Batch: 750 Loss: 2.1055941581726074 Tokens per Sec: 7239.9189453125\n",
      "Epoch: 7 / 20 Batch: 1000 Loss: 2.283325433731079 Tokens per Sec: 7050.57421875\n",
      ">>>>> Evaluate loss 2.3288581371307373\n",
      "One epoch cost 1:42:44.919380.\n",
      "\n",
      "Epoch: 8 / 20 Batch: 7500 Loss: 2.4145169258117676 Tokens per Sec: 5976.71484375\n",
      "Epoch: 8 / 20 Batch: 15000 Loss: 2.641356945037842 Tokens per Sec: 5975.43359375\n",
      "Epoch: 8 / 20 Batch: 22500 Loss: 2.5570366382598877 Tokens per Sec: 6089.3212890625\n",
      "Epoch: 8 / 20 Batch: 30000 Loss: 2.3391661643981934 Tokens per Sec: 5763.10009765625\n",
      ">>>>> Evaluate...\n",
      "Epoch: 8 / 20 Batch: 250 Loss: 2.08655047416687 Tokens per Sec: 6815.71533203125\n",
      "Epoch: 8 / 20 Batch: 500 Loss: 2.662637710571289 Tokens per Sec: 7478.9072265625\n",
      "Epoch: 8 / 20 Batch: 750 Loss: 2.072500705718994 Tokens per Sec: 6723.4267578125\n",
      "Epoch: 8 / 20 Batch: 1000 Loss: 2.2806849479675293 Tokens per Sec: 6502.255859375\n",
      ">>>>> Evaluate loss 2.3076725006103516\n",
      "One epoch cost 1:45:39.973735.\n",
      "\n",
      "Epoch: 9 / 20 Batch: 7500 Loss: 2.3921468257904053 Tokens per Sec: 5767.1328125\n",
      "Epoch: 9 / 20 Batch: 15000 Loss: 2.5906686782836914 Tokens per Sec: 5706.07080078125\n",
      "Epoch: 9 / 20 Batch: 22500 Loss: 2.510225772857666 Tokens per Sec: 5682.24560546875\n",
      "Epoch: 9 / 20 Batch: 30000 Loss: 2.3287837505340576 Tokens per Sec: 5776.55517578125\n",
      ">>>>> Evaluate...\n",
      "Epoch: 9 / 20 Batch: 250 Loss: 2.0667450428009033 Tokens per Sec: 7592.6982421875\n",
      "Epoch: 9 / 20 Batch: 500 Loss: 2.637509346008301 Tokens per Sec: 6847.54443359375\n",
      "Epoch: 9 / 20 Batch: 750 Loss: 2.0307071208953857 Tokens per Sec: 6873.490234375\n",
      "Epoch: 9 / 20 Batch: 1000 Loss: 2.2696127891540527 Tokens per Sec: 7312.125\n",
      ">>>>> Evaluate loss 2.287026882171631\n",
      "One epoch cost 1:49:14.698993.\n",
      "\n",
      "Epoch: 10 / 20 Batch: 7500 Loss: 2.3842365741729736 Tokens per Sec: 5957.24951171875\n",
      "Epoch: 10 / 20 Batch: 15000 Loss: 2.5715785026550293 Tokens per Sec: 5966.3984375\n",
      "Epoch: 10 / 20 Batch: 22500 Loss: 2.5095009803771973 Tokens per Sec: 5923.98828125\n",
      "Epoch: 10 / 20 Batch: 30000 Loss: 2.2864766120910645 Tokens per Sec: 5925.3544921875\n",
      ">>>>> Evaluate...\n",
      "Epoch: 10 / 20 Batch: 250 Loss: 2.043189287185669 Tokens per Sec: 6901.66064453125\n",
      "Epoch: 10 / 20 Batch: 500 Loss: 2.6199135780334473 Tokens per Sec: 6858.94091796875\n",
      "Epoch: 10 / 20 Batch: 750 Loss: 2.0331294536590576 Tokens per Sec: 7414.05224609375\n",
      "Epoch: 10 / 20 Batch: 1000 Loss: 2.231215238571167 Tokens per Sec: 6599.3896484375\n",
      ">>>>> Evaluate loss 2.269678831100464\n",
      "One epoch cost 1:45:38.825706.\n",
      "\n",
      "Epoch: 11 / 20 Batch: 7500 Loss: 2.376102924346924 Tokens per Sec: 5974.2333984375\n",
      "Epoch: 11 / 20 Batch: 15000 Loss: 2.5259361267089844 Tokens per Sec: 5959.71044921875\n",
      "Epoch: 11 / 20 Batch: 22500 Loss: 2.4567530155181885 Tokens per Sec: 5926.89404296875\n",
      "Epoch: 11 / 20 Batch: 30000 Loss: 2.2665605545043945 Tokens per Sec: 5906.591796875\n",
      ">>>>> Evaluate...\n",
      "Epoch: 11 / 20 Batch: 250 Loss: 2.0233211517333984 Tokens per Sec: 6906.94384765625\n",
      "Epoch: 11 / 20 Batch: 500 Loss: 2.6035683155059814 Tokens per Sec: 7499.138671875\n",
      "Epoch: 11 / 20 Batch: 750 Loss: 2.010946273803711 Tokens per Sec: 6939.361328125\n",
      "Epoch: 11 / 20 Batch: 1000 Loss: 2.231934070587158 Tokens per Sec: 6601.16796875\n",
      ">>>>> Evaluate loss 2.253793478012085\n",
      "One epoch cost 1:45:37.049532.\n",
      "\n",
      "Epoch: 12 / 20 Batch: 7500 Loss: 2.324247121810913 Tokens per Sec: 5952.54150390625\n",
      "Epoch: 12 / 20 Batch: 15000 Loss: 2.5323326587677 Tokens per Sec: 5962.97607421875\n",
      "Epoch: 12 / 20 Batch: 22500 Loss: 2.463430881500244 Tokens per Sec: 5928.83203125\n",
      "Epoch: 12 / 20 Batch: 30000 Loss: 2.2615575790405273 Tokens per Sec: 5961.1533203125\n",
      ">>>>> Evaluate...\n",
      "Epoch: 12 / 20 Batch: 250 Loss: 2.021310806274414 Tokens per Sec: 7004.109375\n",
      "Epoch: 12 / 20 Batch: 500 Loss: 2.6024513244628906 Tokens per Sec: 7501.5302734375\n",
      "Epoch: 12 / 20 Batch: 750 Loss: 1.9941877126693726 Tokens per Sec: 6837.427734375\n",
      "Epoch: 12 / 20 Batch: 1000 Loss: 2.216257333755493 Tokens per Sec: 6665.49365234375\n",
      ">>>>> Evaluate loss 2.2402167320251465\n",
      "One epoch cost 1:45:29.912547.\n",
      "\n",
      "Epoch: 13 / 20 Batch: 7500 Loss: 2.3195323944091797 Tokens per Sec: 5963.5615234375\n",
      "Epoch: 13 / 20 Batch: 15000 Loss: 2.505580186843872 Tokens per Sec: 5963.8681640625\n",
      "Epoch: 13 / 20 Batch: 22500 Loss: 2.455225944519043 Tokens per Sec: 5939.06640625\n",
      "Epoch: 13 / 20 Batch: 30000 Loss: 2.2543578147888184 Tokens per Sec: 5946.232421875\n",
      ">>>>> Evaluate...\n",
      "Epoch: 13 / 20 Batch: 250 Loss: 2.007572889328003 Tokens per Sec: 7562.09765625\n",
      "Epoch: 13 / 20 Batch: 500 Loss: 2.5852808952331543 Tokens per Sec: 6870.39892578125\n",
      "Epoch: 13 / 20 Batch: 750 Loss: 1.9856486320495605 Tokens per Sec: 6811.98193359375\n",
      "Epoch: 13 / 20 Batch: 1000 Loss: 2.1898787021636963 Tokens per Sec: 6684.05224609375\n",
      ">>>>> Evaluate loss 2.2282931804656982\n",
      "One epoch cost 1:45:27.416123.\n",
      "\n",
      "Epoch: 14 / 20 Batch: 7500 Loss: 2.2931950092315674 Tokens per Sec: 5981.88720703125\n",
      "Epoch: 14 / 20 Batch: 15000 Loss: 2.507706880569458 Tokens per Sec: 5958.23876953125\n",
      "Epoch: 14 / 20 Batch: 22500 Loss: 2.418402671813965 Tokens per Sec: 5980.71923828125\n",
      "Epoch: 14 / 20 Batch: 30000 Loss: 2.2247233390808105 Tokens per Sec: 6156.34423828125\n",
      ">>>>> Evaluate...\n",
      "Epoch: 14 / 20 Batch: 250 Loss: 1.9982248544692993 Tokens per Sec: 8176.8212890625\n",
      "Epoch: 14 / 20 Batch: 500 Loss: 2.5888545513153076 Tokens per Sec: 7248.54150390625\n",
      "Epoch: 14 / 20 Batch: 750 Loss: 1.9589682817459106 Tokens per Sec: 7272.22412109375\n",
      "Epoch: 14 / 20 Batch: 1000 Loss: 2.189088821411133 Tokens per Sec: 7926.29248046875\n",
      ">>>>> Evaluate loss 2.2166106700897217\n",
      "One epoch cost 1:43:59.907272.\n",
      "\n",
      "Epoch: 15 / 20 Batch: 7500 Loss: 2.2733383178710938 Tokens per Sec: 6109.00830078125\n",
      "Epoch: 15 / 20 Batch: 15000 Loss: 2.528771162033081 Tokens per Sec: 5976.53955078125\n",
      "Epoch: 15 / 20 Batch: 22500 Loss: 2.433776617050171 Tokens per Sec: 5943.79736328125\n",
      "Epoch: 15 / 20 Batch: 30000 Loss: 2.231771945953369 Tokens per Sec: 5944.25390625\n",
      ">>>>> Evaluate...\n",
      "Epoch: 15 / 20 Batch: 250 Loss: 1.9856631755828857 Tokens per Sec: 7415.11962890625\n",
      "Epoch: 15 / 20 Batch: 500 Loss: 2.5754969120025635 Tokens per Sec: 7355.61865234375\n",
      "Epoch: 15 / 20 Batch: 750 Loss: 1.9484378099441528 Tokens per Sec: 7341.59228515625\n",
      "Epoch: 15 / 20 Batch: 1000 Loss: 2.1669178009033203 Tokens per Sec: 7956.78125\n",
      ">>>>> Evaluate loss 2.2089850902557373\n",
      "One epoch cost 1:44:05.111483.\n",
      "\n",
      "Epoch: 16 / 20 Batch: 7500 Loss: 2.261845588684082 Tokens per Sec: 6787.0771484375\n",
      "Epoch: 16 / 20 Batch: 15000 Loss: 2.473114490509033 Tokens per Sec: 6807.41552734375\n",
      "Epoch: 16 / 20 Batch: 22500 Loss: 2.429258108139038 Tokens per Sec: 6270.7978515625\n",
      "Epoch: 16 / 20 Batch: 30000 Loss: 2.178618907928467 Tokens per Sec: 5990.78857421875\n",
      ">>>>> Evaluate...\n",
      "Epoch: 16 / 20 Batch: 250 Loss: 1.9763227701187134 Tokens per Sec: 6988.05517578125\n",
      "Epoch: 16 / 20 Batch: 500 Loss: 2.549022912979126 Tokens per Sec: 6978.87890625\n",
      "Epoch: 16 / 20 Batch: 750 Loss: 1.9386364221572876 Tokens per Sec: 7608.3076171875\n",
      "Epoch: 16 / 20 Batch: 1000 Loss: 2.1630494594573975 Tokens per Sec: 6758.28466796875\n",
      ">>>>> Evaluate loss 2.197813034057617\n",
      "One epoch cost 1:37:55.365799.\n",
      "\n",
      "Epoch: 17 / 20 Batch: 7500 Loss: 2.2789673805236816 Tokens per Sec: 6163.10986328125\n",
      "Epoch: 17 / 20 Batch: 15000 Loss: 2.5068211555480957 Tokens per Sec: 6248.5830078125\n",
      "Epoch: 17 / 20 Batch: 22500 Loss: 2.3826870918273926 Tokens per Sec: 6225.60888671875\n",
      "Epoch: 17 / 20 Batch: 30000 Loss: 2.2021350860595703 Tokens per Sec: 6260.84619140625\n",
      ">>>>> Evaluate...\n",
      "Epoch: 17 / 20 Batch: 250 Loss: 1.955636739730835 Tokens per Sec: 7363.94482421875\n",
      "Epoch: 17 / 20 Batch: 500 Loss: 2.5416464805603027 Tokens per Sec: 8192.6494140625\n",
      "Epoch: 17 / 20 Batch: 750 Loss: 1.9321370124816895 Tokens per Sec: 7280.06201171875\n",
      "Epoch: 17 / 20 Batch: 1000 Loss: 2.15522837638855 Tokens per Sec: 7073.5107421875\n",
      ">>>>> Evaluate loss 2.1912875175476074\n",
      "One epoch cost 1:40:42.153195.\n",
      "\n",
      "Epoch: 18 / 20 Batch: 7500 Loss: 2.290844440460205 Tokens per Sec: 6276.67822265625\n",
      "Epoch: 18 / 20 Batch: 15000 Loss: 2.4804728031158447 Tokens per Sec: 6324.037109375\n",
      "Epoch: 18 / 20 Batch: 22500 Loss: 2.404942274093628 Tokens per Sec: 6289.13671875\n",
      "Epoch: 18 / 20 Batch: 30000 Loss: 2.178884506225586 Tokens per Sec: 6785.3017578125\n",
      ">>>>> Evaluate...\n",
      "Epoch: 18 / 20 Batch: 250 Loss: 1.9563473463058472 Tokens per Sec: 7387.40673828125\n",
      "Epoch: 18 / 20 Batch: 500 Loss: 2.5586953163146973 Tokens per Sec: 8235.5498046875\n",
      "Epoch: 18 / 20 Batch: 750 Loss: 1.9177767038345337 Tokens per Sec: 7230.83544921875\n",
      "Epoch: 18 / 20 Batch: 1000 Loss: 2.1495680809020996 Tokens per Sec: 7123.08056640625\n",
      ">>>>> Evaluate loss 2.1846346855163574\n",
      "One epoch cost 1:37:44.552111.\n",
      "\n",
      "Epoch: 19 / 20 Batch: 7500 Loss: 2.26311993598938 Tokens per Sec: 6788.3330078125\n",
      "Epoch: 19 / 20 Batch: 15000 Loss: 2.4594714641571045 Tokens per Sec: 6090.470703125\n",
      "Epoch: 19 / 20 Batch: 22500 Loss: 2.348912477493286 Tokens per Sec: 6381.58056640625\n",
      "Epoch: 19 / 20 Batch: 30000 Loss: 2.1361160278320312 Tokens per Sec: 6678.49951171875\n",
      ">>>>> Evaluate...\n",
      "Epoch: 19 / 20 Batch: 250 Loss: 1.9465745687484741 Tokens per Sec: 8213.6044921875\n",
      "Epoch: 19 / 20 Batch: 500 Loss: 2.5327329635620117 Tokens per Sec: 7254.8837890625\n",
      "Epoch: 19 / 20 Batch: 750 Loss: 1.9298135042190552 Tokens per Sec: 7097.05615234375\n",
      "Epoch: 19 / 20 Batch: 1000 Loss: 2.1440768241882324 Tokens per Sec: 7008.23046875\n",
      ">>>>> Evaluate loss 2.177417278289795\n",
      "One epoch cost 1:37:11.987443.\n",
      "\n",
      "Epoch: 20 / 20 Batch: 7500 Loss: 2.2458181381225586 Tokens per Sec: 6269.39990234375\n",
      "Epoch: 20 / 20 Batch: 15000 Loss: 2.472587823867798 Tokens per Sec: 6229.765625\n",
      "Epoch: 20 / 20 Batch: 22500 Loss: 2.372598886489868 Tokens per Sec: 6319.93115234375\n",
      "Epoch: 20 / 20 Batch: 30000 Loss: 2.1819775104522705 Tokens per Sec: 6275.71240234375\n",
      ">>>>> Evaluate...\n",
      "Epoch: 20 / 20 Batch: 250 Loss: 1.9408795833587646 Tokens per Sec: 7922.15869140625\n",
      "Epoch: 20 / 20 Batch: 500 Loss: 2.5210659503936768 Tokens per Sec: 7215.458984375\n",
      "Epoch: 20 / 20 Batch: 750 Loss: 1.9279927015304565 Tokens per Sec: 7166.41455078125\n",
      "Epoch: 20 / 20 Batch: 1000 Loss: 2.1380398273468018 Tokens per Sec: 7844.13037109375\n",
      ">>>>> Evaluate loss 2.169428825378418\n",
      "One epoch cost 1:40:05.021539.\n",
      "\n",
      "<<<<<<<<<< Finished training, cost 1 day, 10:53:25.273453.\n"
     ]
    }
   ],
   "source": [
    "train(data, model, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16.Prediction or say Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    '''\n",
    "    predict tgtids from src(one sentence)\n",
    "    '''\n",
    "    # encode src\n",
    "    # memory = model.encode(\n",
    "    memory = model.module.encode(\n",
    "        src=src,\n",
    "        src_mask=src_mask\n",
    "    )\n",
    "    preds = torch.ones(1, 1).fill_(start_symbol).type_as(src.data) # 1*1, fill id of '<BOS>', LongTensor\n",
    "    for _ in range(max_len-1):\n",
    "        # decode src, tgt\n",
    "        # out = model.decode(\n",
    "        out = model.module.decode(\n",
    "            memory=memory,\n",
    "            src_mask=src_mask,\n",
    "            tgt=Variable(preds),\n",
    "            tgt_mask=Variable(subsequent_mask(preds.size(1)).type_as(src.data))\n",
    "        )\n",
    "        # hidden repretation -> probability of log_softmax for words in dict\n",
    "        prob = model.module.generator(out[:, -1]) # excluding '<EOS>'\n",
    "        # prob = model.generator(out[:, -1]) # excluding '<EOS>'\n",
    "        # get next prediction id of current position max probability\n",
    "        _, next_word_id = torch.max(prob, dim=1) # [id]\n",
    "        next_word_id = next_word_id.data[0] # id\n",
    "        preds = torch.cat([preds, torch.ones(1, 1).type_as(src.data).fill_(next_word_id)], dim=1)\n",
    "\n",
    "    return preds\n",
    "\n",
    "\n",
    "def translate(data, model):\n",
    "    '''\n",
    "    predict tgts id and print words\n",
    "    '''\n",
    "    fw = open('./translation_L.txt', 'a', encoding=\"utf-8\")\n",
    "    print(\">>>>>>>>>> Start translating\\n\")\n",
    "    fw.write(\">>>>>>>>>> Start translating\\n\\n\")\n",
    "    translate_start = datetime.now()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        total_BLEU4 = 0\n",
    "        total_BLEU2 = 0\n",
    "        len_dev = len(data.dev_en_id)\n",
    "        cost_time = datetime.now() - datetime.now()\n",
    "\n",
    "        for i in range(len_dev):\n",
    "            one_translate_start = datetime.now()\n",
    "            # print(f'idx / len_dev: {i+1} / {len_dev}')\n",
    "            fw.write(f'idx / len_dev: {i+1} / {len_dev}\\n')\n",
    "\n",
    "            # source sentence\n",
    "            src_sent_list = [data.en_index_dict[w] for w in data.dev_en_id[i]]\n",
    "            src_sent = ' '.join(src_sent_list)\n",
    "            # print(f'source_sentence: {src_sent}')\n",
    "            fw.write(f'source_sentence: {src_sent}\\n')\n",
    "\n",
    "            # target sentence\n",
    "            tgt_sent_list = [data.cn_index_dict[w] for w in data.dev_cn_id[i]]\n",
    "            tgt_sent = ' '.join(tgt_sent_list)\n",
    "            # print(f'target_sentence: {tgt_sent}')\n",
    "            fw.write(f'target_sentence: {tgt_sent}\\n')\n",
    "\n",
    "            # translate\n",
    "            src = torch.from_numpy(np.array(data.dev_en_id[i])).long().cuda().unsqueeze(0)\n",
    "            src_mask = (src != 0).unsqueeze(-2)\n",
    "            out = greedy_decode(\n",
    "                model=model,\n",
    "                src=src,\n",
    "                src_mask=src_mask,\n",
    "                max_len=MAX_LENGTH,\n",
    "                start_symbol=data.cn_word_dict['<BOS>']\n",
    "            )\n",
    "            translation = [] # for one sentence\n",
    "            for j in range(1, out.size(1)):\n",
    "                sym = data.cn_index_dict[out[0, j].item()]\n",
    "                if sym != '<EOS>': translation.append(sym)\n",
    "                else: break\n",
    "            # print(f'translation: {\" \".join(translation)}')\n",
    "            fw.write(f'translation: {\" \".join(translation)}\\n')\n",
    "\n",
    "            # compute BLEU\n",
    "            # 4-gram cumulative BLEU\n",
    "            # >>> from nltk.translate.bleu_score import sentence_bleu\n",
    "            # >>> reference = [['this', 'is', 'small', 'test']]\n",
    "            # >>> candidate = ['this', 'is', 'a', 'test']\n",
    "            # >>> score = sentence_bleu(reference, candidate)\n",
    "            score4 = sentence_bleu([tgt_sent_list], translation)\n",
    "            score2 = sentence_bleu([tgt_sent_list], translation, weights=(0.75, 0.25, 0, 0))\n",
    "            total_BLEU4 += score4\n",
    "            total_BLEU2 += score2\n",
    "            # print(f'BLEU score4: {score4}')\n",
    "            # print(f'BLEU score2: {score2}\\n')\n",
    "            fw.write(f'BLEU score4: {score4}\\n')\n",
    "            fw.write(f'BLEU score2: {score2}\\n\\n')\n",
    "            cost_time += datetime.now() - one_translate_start\n",
    "\n",
    "            freq = 1e3\n",
    "            if (i+1) % freq == 0:\n",
    "                print(f'Translation cost {cost_time} per {freq} sentences.\\n')\n",
    "                fw.write(f'Translation cost {cost_time} per {freq} sentences.\\n\\n')\n",
    "                cost_time = datetime.now() - datetime.now()\n",
    "\n",
    "        avg_BLEU4 = total_BLEU4 / len_dev\n",
    "        avg_BLEU2 = total_BLEU2 / len_dev\n",
    "        print(f'average_BLEU4: {avg_BLEU4}')\n",
    "        print(f'average_BLEU2: {avg_BLEU2}')\n",
    "        fw.write(f'average_BLEU4: {avg_BLEU4}\\n')\n",
    "        fw.write(f'average_BLEU2: {avg_BLEU2}\\n')\n",
    "\n",
    "    print(f'<<<<<<<<<< Finished translating, cost {datetime.now()-translate_start}.')\n",
    "    fw.write(f'<<<<<<<<<< Finished translating, cost {datetime.now()-translate_start}.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>>>> Start translating\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\siat\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\siat\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\siat\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation cost 0:10:52.183882 per 100 sentences.\n",
      "\n",
      "Translation cost 0:10:49.735061 per 100 sentences.\n",
      "\n",
      "Translation cost 0:10:52.295248 per 100 sentences.\n",
      "\n",
      "Translation cost 0:11:05.141149 per 100 sentences.\n",
      "\n",
      "Translation cost 0:11:02.627910 per 100 sentences.\n",
      "\n",
      "Translation cost 0:11:03.319644 per 100 sentences.\n",
      "\n",
      "Translation cost 0:11:02.429118 per 100 sentences.\n",
      "\n",
      "Translation cost 0:11:05.408113 per 100 sentences.\n",
      "\n",
      "Translation cost 0:10:49.384477 per 100 sentences.\n",
      "\n",
      "Translation cost 0:10:56.427287 per 100 sentences.\n",
      "\n",
      "Translation cost 0:10:54.737975 per 100 sentences.\n",
      "\n",
      "Translation cost 0:10:55.954749 per 100 sentences.\n",
      "\n",
      "Translation cost 0:10:56.442199 per 100 sentences.\n",
      "\n",
      "Translation cost 0:10:56.437761 per 100 sentences.\n",
      "\n",
      "Translation cost 0:10:52 per 100 sentences.\n",
      "\n",
      "Translation cost 0:10:48.209595 per 100 sentences.\n",
      "\n",
      "Translation cost 0:10:48.208636 per 100 sentences.\n",
      "\n",
      "Translation cost 0:10:48.318227 per 100 sentences.\n",
      "\n",
      "Translation cost 0:10:48.208595 per 100 sentences.\n",
      "\n",
      "Translation cost 0:10:53.398315 per 100 sentences.\n",
      "\n",
      "Translation cost 0:10:59.415840 per 100 sentences.\n",
      "\n",
      "Translation cost 0:10:58.128715 per 100 sentences.\n",
      "\n",
      "Translation cost 0:11:08.800765 per 100 sentences.\n",
      "\n",
      "Translation cost 0:11:20.652638 per 100 sentences.\n",
      "\n",
      "Translation cost 0:10:55.811353 per 100 sentences.\n",
      "\n",
      "Translation cost 0:10:47.611388 per 100 sentences.\n",
      "\n",
      "Translation cost 0:10:53.515406 per 100 sentences.\n",
      "\n",
      "Translation cost 0:10:55.429093 per 100 sentences.\n",
      "\n",
      "Translation cost 0:10:54.765513 per 100 sentences.\n",
      "\n",
      "Translation cost 0:11:08.252407 per 100 sentences.\n",
      "\n",
      "Translation cost 0:11:08.953123 per 100 sentences.\n",
      "\n",
      "Translation cost 0:11:12.351910 per 100 sentences.\n",
      "\n",
      "Translation cost 0:11:01.086565 per 100 sentences.\n",
      "\n",
      "Translation cost 0:11:05.660487 per 100 sentences.\n",
      "\n",
      "Translation cost 0:11:09.285161 per 100 sentences.\n",
      "\n",
      "Translation cost 0:11:18.182078 per 100 sentences.\n",
      "\n",
      "Translation cost 0:11:13.363061 per 100 sentences.\n",
      "\n",
      "Translation cost 0:10:59.435682 per 100 sentences.\n",
      "\n",
      "Translation cost 0:11:08.079940 per 100 sentences.\n",
      "\n",
      "average_BLEU4: 0.15523852134722\n",
      "average_BLEU2: 0.3785954391863779\n",
      "<<<<<<<<<< Finished translating, cost 7:12:15.516085.\n"
     ]
    }
   ],
   "source": [
    "# load the weights of model\n",
    "model.load_state_dict(torch.load(SAVE_FILE))\n",
    "# translation\n",
    "translate(data, model)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6ec20a056812a08efc2a86552901d750b383cff57cd8ce6e27533f89d7039064"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
